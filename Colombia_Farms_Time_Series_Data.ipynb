{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colombia_Farms_Time_Series_Data.ipynb",
      "provenance": [],
      "mount_file_id": "1d1LzuRUWI-EijmC-vaXdM92EAPWMhSvJ",
      "authorship_tag": "ABX9TyNj8DsgbnyRuh1DA9CuLA5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Max-FM/SPRINT-Colombia/blob/main/Colombia_Farms_Time_Series_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruk4Ycv3XHvS"
      },
      "source": [
        "#Colombia Farms Time Series Data\n",
        "\n",
        "**Disclaimer:** This notebook is partially adapted from [this](https://colab.research.google.com/github/google/earthengine-community/blob/master/tutorials/time-series-visualization-with-altair/tutorial.ipynb) tutorial on visualising time-series data using Google Earth Engine and Python. This is a good reference to further understand how this notebook works.\n",
        "\n",
        "**Note:** This notebook will not work straight 'out of the box' and will require a bit of editing to suit your purposes. The idea of this notebook is to show the method I used to obtain time-series data from Google Earth Engine and is intended to be used as a template for future work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68_R807C3qLZ"
      },
      "source": [
        "##Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9FdeMqVZCYU"
      },
      "source": [
        "import ee\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tubT3goC3ua3"
      },
      "source": [
        "##Authenticate Google Earth Engine\n",
        "\n",
        "To access the Google Earth Engine API you require an account. To request access, go to [https://signup.earthengine.google.com](https://signup.earthengine.google.com/). You may have to wait up to a day or so to be granted access and it's possible you will not recieve any email communication. To manually check whether you have access, try to log into [https://code.earthengine.google.com](https://code.earthengine.google.com/), or attempt to run the next cell and follow the instructions provided in the output cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-uLIrdLZRoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4aa801-d2c7-463d-8c4d-915a1a30ed7e"
      },
      "source": [
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "ee.Initialize()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=pYIJH4U7NI4Ays6wgTNXJfOfTg-6QlunEYb-w3DtTK0&code_challenge_method=S256\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below. \n",
            "Enter verification code: 4/1AY0e-g4-WJFlOJDECSeBcExu1vkl4Ra2l0MXz68PmeuDtbUrZn6gYkPHARE\n",
            "\n",
            "Successfully saved authorization token.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGrgSnJuXmk6"
      },
      "source": [
        "## Define Request Function\n",
        "\n",
        "This function contains a series of image collections on Google Earth Engine that we wish to obtain time series data from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDmWaq6BXybF"
      },
      "source": [
        "def calculate_RVI(image):\n",
        "    bandDict = {\n",
        "                'VV': image.select('VV'), \n",
        "                'VH': image.select('VH')\n",
        "                }\n",
        "\n",
        "    rvi = image.expression('(4*VH)/(VH+VV)', bandDict) \\\n",
        "               .rename('RVI')\n",
        "\n",
        "    return image.addBands(rvi)\n",
        "\n",
        "def get_Sentinel_1_RVI(criteria):\n",
        "    SENTINEL_1_SAR_GRD_C_BAND = ee.ImageCollection(\"COPERNICUS/S1_GRD\") \\\n",
        "                                  .filter(criteria) \\\n",
        "                                  .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "                                  .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH')) \\\n",
        "                                  .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
        "                                  .select(['VV', 'VH'])\n",
        "\n",
        "    SENTINEL_1_RVI = SENTINEL_1_SAR_GRD_C_BAND.map(calculate_RVI).select('RVI')\n",
        "\n",
        "    return SENTINEL_1_RVI\n",
        "\n",
        "def obtain_data(region):\n",
        "    criteria  = ee.Filter.geometry(region) \n",
        "\n",
        "    SENTINEL_1_RVI = get_Sentinel_1_RVI(criteria)\n",
        "\n",
        "    MODIS_16D_TERRA_VEG = ee.ImageCollection(\"MODIS/006/MOD13Q1\") \\\n",
        "                             .filter(criteria) \\\n",
        "                             .select(['NDVI', 'EVI'])\n",
        "\n",
        "    MODIS_16D_AQUA_VEG = ee.ImageCollection(\"MODIS/006/MYD13Q1\") \\\n",
        "                           .filter(criteria) \\\n",
        "                           .select(['NDVI', 'EVI'])\n",
        "\n",
        "    TERRA_CLIMATE_MONTHLY = ee.ImageCollection(\"IDAHO_EPSCOR/TERRACLIMATE\") \\\n",
        "                              .filter(criteria)\n",
        "\n",
        "    PERSIANN_DAILY = ee.ImageCollection(\"NOAA/PERSIANN-CDR\") \\\n",
        "                       .filter(criteria)\n",
        "                       \n",
        "    CHIRPS_DAILY = ee.ImageCollection(\"UCSB-CHG/CHIRPS/DAILY\") \\\n",
        "                     .filter(criteria)\n",
        "\n",
        "    IMERG_MONTHLY = ee.ImageCollection(\"NASA/GPM_L3/IMERG_MONTHLY_V06\") \\\n",
        "                      .filter(criteria)\n",
        "\n",
        "    ERA5_DAILY = ee.ImageCollection(\"ECMWF/ERA5/DAILY\") \\\n",
        "                   .filter(criteria)\n",
        "\n",
        "    ERA5_MONTHLY = ee.ImageCollection(\"ECMWF/ERA5/MONTHLY\") \\\n",
        "                     .filter(criteria)\n",
        "\n",
        "    # Above image collections are placed in a Python dictionary for convenience.\n",
        "    image_collections = {\n",
        "                         'SENTINEL_1_RVI': SENTINEL_1_RVI,\n",
        "                         'MODIS_16D_TERRA_VEG': MODIS_16D_TERRA_VEG,\n",
        "                         'MODIS_16D_AQUA_VEG': MODIS_16D_AQUA_VEG,\n",
        "                         'TERRA_CLIMATE_MONTHLY': TERRA_CLIMATE_MONTHLY,\n",
        "                         'PERSIANN_DAILY': PERSIANN_DAILY,\n",
        "                         'CHIRPS_DAILY': CHIRPS_DAILY,\n",
        "                         'IMERG_MONTHLY': IMERG_MONTHLY,\n",
        "                         'ERA5_DAILY': ERA5_DAILY,\n",
        "                         'ERA5_MONTHLY': ERA5_MONTHLY\n",
        "                         }\n",
        "\n",
        "    return image_collections"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUufiuUk8Wrk"
      },
      "source": [
        "## Define Region Reduction and Data Collection Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJgSyFoX8FcZ"
      },
      "source": [
        "def create_reduce_region_function(\n",
        "                                  geometry,\n",
        "                                  reducer=ee.Reducer.median(),\n",
        "                                  scale=250,\n",
        "                                  crs='EPSG:4326',\n",
        "                                  bestEffort=True,\n",
        "                                  maxPixels=1e13,\n",
        "                                  tileScale=4\n",
        "                                  ):\n",
        "  \"\"\"Creates a region reduction function.\n",
        "\n",
        "  Creates a region reduction function intended to be used as the input function\n",
        "  to ee.ImageCollection.map() for reducing pixels intersecting a provided region\n",
        "  to a statistic for each image in a collection. See ee.Image.reduceRegion()\n",
        "  documentation for more details.\n",
        "\n",
        "  Args:\n",
        "    geometry:\n",
        "      An ee.Geometry that defines the region over which to reduce data.\n",
        "    reducer:\n",
        "      Optional; An ee.Reducer that defines the reduction method.\n",
        "    scale:\n",
        "      Optional; A number that defines the nominal scale in meters of the\n",
        "      projection to work in.\n",
        "    crs:\n",
        "      Optional; An ee.Projection or EPSG string ('EPSG:5070') that defines\n",
        "      the projection to work in.\n",
        "    bestEffort:\n",
        "      Optional; A Boolean indicator for whether to use a larger scale if the\n",
        "      geometry contains too many pixels at the given scale for the operation\n",
        "      to succeed.\n",
        "    maxPixels:\n",
        "      Optional; A number specifying the maximum number of pixels to reduce.\n",
        "    tileScale:\n",
        "      Optional; A number representing the scaling factor used to reduce\n",
        "      aggregation tile size; using a larger tileScale (e.g. 2 or 4) may enable\n",
        "      computations that run out of memory with the default.\n",
        "\n",
        "  Returns:\n",
        "    A function that accepts an ee.Image and reduces it by region, according to\n",
        "    the provided arguments.\n",
        "  \"\"\"\n",
        "\n",
        "  def reduce_region_function(img):\n",
        "    \"\"\"Applies the ee.Image.reduceRegion() method.\n",
        "\n",
        "    Args:\n",
        "      img:\n",
        "        An ee.Image to reduce to a statistic by region.\n",
        "\n",
        "    Returns:\n",
        "      An ee.Feature that contains properties representing the image region\n",
        "      reduction results per band and the image timestamp formatted as\n",
        "      milliseconds from Unix epoch (included to enable time series plotting).\n",
        "    \"\"\"\n",
        "\n",
        "    stat = img.reduceRegion(\n",
        "                            reducer=reducer,\n",
        "                            geometry=geometry,\n",
        "                            scale=scale,\n",
        "                            crs=crs,\n",
        "                            bestEffort=bestEffort,\n",
        "                            maxPixels=maxPixels,\n",
        "                            tileScale=tileScale\n",
        "                            )\n",
        "\n",
        "    return ee.Feature(geometry, stat).set({'millis': img.date().millis()})\n",
        "  return reduce_region_function\n",
        "\n",
        "# Define a function to transfer feature properties to a dictionary.\n",
        "def feature_collection_to_dict(fc):\n",
        "  prop_names = fc.first().propertyNames()\n",
        "  prop_lists = fc.reduceColumns(\n",
        "      reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
        "      selectors=prop_names).get('list')\n",
        "\n",
        "  return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
        "\n",
        "# Function to add date variables to DataFrame.\n",
        "def add_date_info(df):\n",
        "  df['Timestamp'] = pd.to_datetime(df['millis'], unit='ms')\n",
        "  df['Year'] = pd.DatetimeIndex(df['Timestamp']).year\n",
        "  df['Month'] = pd.DatetimeIndex(df['Timestamp']).month\n",
        "  df['Day'] = pd.DatetimeIndex(df['Timestamp']).day\n",
        "  df['DOY'] = pd.DatetimeIndex(df['Timestamp']).dayofyear\n",
        "  return df\n",
        "\n",
        "# Aggregates pixel values over a defined region in a Google Earth Engine image \n",
        "# collection and returns a time-series Pandas dataframe.\n",
        "def obtain_timeseries_dataframe(\n",
        "                                image_collection, \n",
        "                                region, \n",
        "                                aggregation, \n",
        "                                scale=250,\n",
        "                                crs='EPSG:4326',\n",
        "                                bestEffort=True,\n",
        "                                maxPixels=1e13,\n",
        "                                tileScale=4\n",
        "                                ):\n",
        "    \n",
        "    reduce_dict = {'median': ee.Reducer.median(),\n",
        "                   'stdDev': ee.Reducer.stdDev(),\n",
        "                   'count': ee.Reducer.count()}\n",
        "    \n",
        "    # Defines the reduction function to apply to the image collection.\n",
        "    reduce_fn = create_reduce_region_function(geometry=region, \n",
        "                                              reducer=reduce_dict[aggregation], \n",
        "                                              scale=scale,\n",
        "                                              crs=crs,\n",
        "                                              bestEffort=bestEffort,\n",
        "                                              maxPixels=maxPixels,\n",
        "                                              tileScale=tileScale)\n",
        "    \n",
        "    # Creates a time-series feature collection by aggregating over the image \n",
        "    # collection.\n",
        "    feature_collection_reduced = ee.FeatureCollection(image_collection.map(reduce_fn)) \\\n",
        "                                   .filter(ee.Filter.notNull(image_collection.first().bandNames()))\n",
        "\n",
        "    # Converts the feature collection to a Python dictionary.\n",
        "    reduced_dict = feature_collection_to_dict(feature_collection_reduced).getInfo()\n",
        "\n",
        "    # Converts the Python dictionary to a Pandas dataframe.\n",
        "    df = pd.DataFrame(reduced_dict)\n",
        "\n",
        "    # Renames data columms to reflect aggregation type.\n",
        "    df.columns = df.columns.map(lambda colName : colName+f'_{aggregation}' if colName !='system:index' and colName !='millis' else colName)\n",
        "    # Drops the 'millis' column.\n",
        "    df = add_date_info(df).drop(columns=['millis'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Filters an image collection to within a certain date range.\n",
        "def filter_collection_by_date(collection, start_date, end_date):\n",
        "    start_date = ee.Date(start_date)\n",
        "    end_date = ee.Date (end_date)\n",
        "\n",
        "    date_filter = ee.Filter.date(start_date, end_date)\n",
        "\n",
        "    collection_filtered = collection.filter(date_filter)\n",
        "\n",
        "    return collection_filtered\n",
        "\n",
        "# Sorts the columns alphabetically with the exeption of a pre-detemined list of\n",
        "# columns.\n",
        "def reorder_dataframe(df, preordered_columns):\n",
        "    df = df.reindex(sorted(df.columns), axis=1)\n",
        "    new_order = preordered_columns + [c for c in df.columns if c not in preordered_columns]\n",
        "    df = df.reindex(columns=new_order)\n",
        "    return df\n",
        "\n",
        "def add_info_columns(df, district_name, farm_name, collection_name):\n",
        "    df['District'] = district_name\n",
        "    df['Farm'] = farm_name\n",
        "    df['Image_Collection'] = collection_name\n",
        "\n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir-rp8vX9vpu"
      },
      "source": [
        "## Testing the Code on a Single Farm\n",
        "\n",
        "Have commented this section out for now. Feel free to uncomment and play around with if you wish to test the methods on a single farm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wK-7_Rn9vPX"
      },
      "source": [
        "# farm = ee.FeatureCollection(f'users/maxfoxley-marrable/SPRINT/Farms/Dosquebradas_Central_N_Farm_C')\n",
        "# aoi = farm.geometry()\n",
        "\n",
        "# start_date = '2019-01-01'\n",
        "# end_date = '2020-01-01'\n",
        "\n",
        "# image_collections = obtain_data(region=aoi)\n",
        "\n",
        "\n",
        "# collection = image_collections['TERRA_CLIMATE_MONTHLY']\n",
        "# collection = filter_collection_by_date(collection, start_date, end_date)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_9MyvRrogoT"
      },
      "source": [
        "# %%time\n",
        "\n",
        "# scale = 10 # If possible, downsample to pixel resolution in metres.\n",
        "\n",
        "# median_df = obtain_timeseries_dataframe(collection,\n",
        "#                                         region=aoi, \n",
        "#                                         aggregation='median', \n",
        "#                                         scale=scale)\n",
        "\n",
        "# std_dev_df = obtain_timeseries_dataframe(collection,\n",
        "#                                          region=aoi, \n",
        "#                                          aggregation='stdDev', \n",
        "#                                          scale=scale)\n",
        "\n",
        "# count_df = obtain_timeseries_dataframe(collection,\n",
        "#                                        region=aoi, \n",
        "#                                        aggregation='count', \n",
        "#                                        scale=scale)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SqbstkjlYA3"
      },
      "source": [
        "# display(median_df.head(), median_df.info())\n",
        "# display(std_dev_df.head())\n",
        "# display(count_df.head())\n",
        "\n",
        "# columns_of_interest = ['system:index', 'Timestamp', 'Year', 'Month', 'Day', 'DOY']\n",
        "\n",
        "# merge = pd.merge_ordered(median_df, std_dev_df, on=columns_of_interest)\n",
        "# merge = reorder_dataframe(merge, columns_of_interest)\n",
        "# merge['Pixel_Count'] = count_df.iloc[:,0]\n",
        "\n",
        "# merge\n",
        "# # merge.sample(30)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5mBKGbdY1NX"
      },
      "source": [
        "##Getting Data for Multiple Farms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26a6sFQ2AZeW"
      },
      "source": [
        "# Creates sub-folders for each farm.\n",
        "\n",
        "# for farm in farms_list:\n",
        "#     district_name = farm.partition('_')[0]\n",
        "#     farm_name = farm.partition('_')[-1]\n",
        "#     dir_name = f'/content/drive/Shared drives/Colombia SPRINT/Test Districts/{district_name}/Farms/CSV Data/{farm_name}'\n",
        "    \n",
        "#     !mkdir '{dir_name}'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_NM37-QmqLt"
      },
      "source": [
        "# List of feature collections that contain shapefiles for farms.\n",
        "farms_list = [\n",
        "              'Dosquebradas_Central_N_Farm_C', \n",
        "              'Dosquebradas_Dos_Cosechas_C',\n",
        "              'Dosquebradas_Fincas_campesinas_C',\n",
        "              'Dosquebradas_La_Union_C',\n",
        "              'Dosquebradas_Las_Camillas_C',\n",
        "              'Dosquebradas_Minas_del_Socorro_C',\n",
        "              'Dosquebradas_NE_area_C',\n",
        "              'Dosquebradas_N_Farm_C',\n",
        "              'Dosquebradas_New_coffee_farm_C',\n",
        "              'Dosquebradas_Santa_Ana_Alto_C',\n",
        "              'Dosquebradas_Santa_Ana_Bajo_La_Union_C',\n",
        "              'Versailles_Barcelona',\n",
        "              'Versailles_El_Chalet',\n",
        "              'Versailles_Guyabel',\n",
        "              'Versailles_La_Primavera',\n",
        "              'Versailles_La_Rivera',\n",
        "              'Versailles_Miraflores',\n",
        "              'Versailles_Predio_Valery',\n",
        "              'Versailles_Roberto',\n",
        "              'Versailles_San_Fernando',\n",
        "              'Versailles_Villa_Emanuel'\n",
        "              ]\n",
        "\n",
        "# Splitting date range into yearly chunks to reduce timeouts on GEE side.\n",
        "date_ranges = [(f'{year}-01-01', f'{year+1}-01-01') for year in np.arange(1997, 2021)]\n",
        "\n",
        "# Resample pixels to resolution in metres. Setting native scale where known.\n",
        "scaleDict = {\n",
        "            'SENTINEL_1_RVI': 10,\n",
        "            'MODIS_16D_TERRA_VEG': 50,\n",
        "            'MODIS_16D_AQUA_VEG': 50, \n",
        "            'TERRA_CLIMATE_MONTHLY': 50,\n",
        "            'PERSIANN_DAILY': 50,\n",
        "            'CHIRPS_DAILY': 50,\n",
        "            'IMERG_MONTHLY': 50,\n",
        "            'ERA5_DAILY': 50,\n",
        "            'ERA5_MONTHLY': 50\n",
        "            }\n",
        "\n",
        "# List of pre-ordered columns to prefix the dataframe with.\n",
        "preordered_columns = ['District', 'Farm', 'Year', 'Month', 'Day', 'DOY', 'Timestamp', 'Image_Collection', 'system:index']\n",
        "# List of columns to merge the median and standard deviation dataframes on.\n",
        "merge_columns = ['system:index', 'Timestamp', 'Year', 'Month', 'Day', 'DOY']\n",
        "\n",
        "# Set to 'None' to iterate through all image collections defined above.\n",
        "# Otherwise provide a list of strings \n",
        "# e.g. ['MODIS_16D_TERRA_VEG', 'PERSIANN_DAILY', etc]\n",
        "desired_collections = None\n",
        "\n",
        "# Set to 'True' to overwrite existing files.\n",
        "overwrite = False"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAjjASWZY64r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620570cf-47ed-43a3-deec-b2e0c801bbf7"
      },
      "source": [
        "%%time\n",
        "import os.path\n",
        "\n",
        "# Loops through the shapefiles for each farm.\n",
        "for farm in farms_list:\n",
        "    # Shape files have been pre-uploaded to Google Earth Engine.\n",
        "    region = ee.FeatureCollection(f'users/maxfoxley-marrable/SPRINT/Farms/{farm}')\n",
        "    aoi = region.geometry()\n",
        "\n",
        "    # Splits the filename string to obtain the district and farm name.\n",
        "    district_name = farm.partition('_')[0]\n",
        "    farm_name = farm.partition('_')[-1]\n",
        "\n",
        "    print(district_name, farm_name)\n",
        "\n",
        "    # Obtains all image collections defined above for said region.\n",
        "    image_collections = obtain_data(region)\n",
        "    \n",
        "    # Filters out unwanted image collections.\n",
        "    if desired_collections:\n",
        "        image_collections = {collection: image_collections[collection] \\\n",
        "                            for collection in desired_collections}\n",
        "\n",
        "    # Loops for each image collection obtained above.\n",
        "    for collection_name, collection in image_collections.items():\n",
        "        \n",
        "        print(collection_name)\n",
        "\n",
        "        # Edit as required. For now I have it directly saving to the shared \n",
        "        # Google Drive.\n",
        "        filepath = f'/content/drive/Shared drives/Colombia SPRINT/Test Districts/{district_name}/Farms/CSV Data/{farm_name}/{farm_name}_{collection_name}.csv'\n",
        "\n",
        "        # Skips if CSV file for a specific region and image collection already \n",
        "        # exists, provided overwrite = False.\n",
        "        if overwrite == False and os.path.isfile(filepath) == True:\n",
        "            print(f'{farm_name}_{collection_name}.csv already exists, skipping.')\n",
        "            continue\n",
        "\n",
        "        # Creates an empty master dataframe which will be iteratively appended \n",
        "        # to create a full ~20 year timeseries. \n",
        "        dataframe = pd.DataFrame()\n",
        "\n",
        "        # Skips the image collection if it contains no images.\n",
        "        if collection.size().getInfo() == 0:\n",
        "            continue\n",
        "        \n",
        "        # Loops though dates in batches of about 8 years to avoid timeouts on\n",
        "        # GEE server side. See above. \n",
        "        for start_date, end_date in date_ranges:\n",
        "            date_range_str = f'{start_date}_{end_date}'\n",
        "\n",
        "            # print(date_range_str)\n",
        "\n",
        "            # Filters image collection to within desired date range.\n",
        "            collection_filtered = filter_collection_by_date(collection, start_date, end_date)\n",
        "\n",
        "            if collection_filtered.size().getInfo() == 0:\n",
        "                continue\n",
        "\n",
        "            # Gets the median pixel values for a region across all images in\n",
        "            # the filtered image collection.\n",
        "            median_df = obtain_timeseries_dataframe(collection_filtered,\n",
        "                                                    region=aoi, \n",
        "                                                    aggregation='median', \n",
        "                                                    scale=scaleDict[collection_name])\n",
        "            \n",
        "            # Same as above, but gets the standard deviation.\n",
        "            std_dev_df = obtain_timeseries_dataframe(collection_filtered,\n",
        "                                                     region=aoi, \n",
        "                                                     aggregation='stdDev', \n",
        "                                                     scale=scaleDict[collection_name])\n",
        "            \n",
        "            # Gets the pixel count for the region, but there has to be a better \n",
        "            # way of pixel counting.\n",
        "            count_df = obtain_timeseries_dataframe(collection_filtered,\n",
        "                                                   region=aoi, \n",
        "                                                   aggregation='count', \n",
        "                                                   scale=scaleDict[collection_name])\n",
        "            \n",
        "\n",
        "            # Merges median and standard deviation time-series data frames into\n",
        "            # a single dataframe.\n",
        "            merge_df = pd.merge_ordered(median_df, std_dev_df, on=merge_columns)\n",
        "            merge_df = add_info_columns(merge_df, district_name, farm_name, collection_name)\n",
        "            merge_df = reorder_dataframe(merge_df, preordered_columns)\n",
        "            merge_df['Pixel_Scale'] = scaleDict[collection_name] # Pixel scale (in metres) defined above.\n",
        "            merge_df['Pixel_Count'] = count_df.iloc[:,0] # Pixel count, this might be a bad way of doing this.\n",
        "                \n",
        "            # Appends merged dataframe for a master dataframe containing the \n",
        "            # full ~20 year time-series.\n",
        "            dataframe = dataframe.append(merge_df, ignore_index=True)\n",
        "\n",
        "        # Saves master dataframe to a CSV file.\n",
        "        dataframe.to_csv(filepath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Versailles Villa_Emanuel\n",
            "SENTINEL_1_RVI\n",
            "MODIS_16D_TERRA_VEG\n",
            "MODIS_16D_AQUA_VEG\n",
            "TERRA_CLIMATE_MONTHLY\n",
            "PERSIANN_DAILY\n",
            "CHIRPS_DAILY\n",
            "IMERG_MONTHLY\n",
            "ERA5_DAILY\n",
            "ERA5_MONTHLY\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}